---
title: "Hoeffding Confidence Intervals"
author: "Matthew Edwards"
date: "11/05/2021"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

## Introduction

Confidence intervals for metric estimates are great for understanding how uncertain your estimates are and for understanding if one model is better than another. 

For classification models there is one confidence interval that can be used for 

- accuracy
- precision
- recall
- sensitivity
- specificity 

that has the advantage of being **exact** and **nonparametric**. This is the Hoeffding Confidence Interval.

## Confidence Interval

An estimate $\hat\theta_n$ of a parameter $\theta$ (e.g. accuracy) has a $(1-\delta)$% confidence interval $\hat\theta_n\pm\epsilon$ if with at least $1-\delta$ probability the statement

$$
\hat\theta_n-\epsilon\leq\theta\leq\hat\theta_n+\epsilon
$$

is true. The probability is with respect to the estimate $\hat\theta_n$ and the error $\epsilon$.

- **Exact**: statement is exact (e.g. non-asymptotic)
- **Nonparametric**: statement holds for (almost) all models of $\hat\theta_n$

## Interpretation 1

Assume we have some model with an unknown true accuracy $\theta$ and 100 independent test sets of size $n$. For each test set $i$ we:

 1. Estimate the accuracy of the model $\hat\theta_{n,i}$
 2. Calculate the error $\epsilon_i$
 3. Construct a 99% confidence interval $CI_i = \hat\theta_{n,i}\pm\epsilon_i$

The phrase "99% confidence" means that out of the 100 confidence intervals we should expect at least 99 (which is 99% of the 100) to contain the unknown true accuracy, $\theta\in CI_i$.

## Interpretation 2

Approximately 99% confidence.

```{r}
set.seed(42)
delta = 0.5
n = 1000
epsilon = sqrt(log(2/delta)/(2*n)) 
delta_bar = log(2/delta)
epsilon = (sqrt(delta_bar*(2*delta_bar+n))+delta_bar)/(sqrt(2)*n)
tibble(test_set = seq_len(100),
       accuracy = replicate(100, mean(rbinom(n, size = 1, prob = 0.92)))) %>% 
  mutate(lower = pmax(accuracy - epsilon, 0)) %>% 
  mutate(upper = pmin(accuracy + epsilon, 1)) %>% 
  ggplot() +
  geom_linerange(aes(x = test_set, y = accuracy, ymin = lower, ymax = upper)) +
  coord_flip() +
  geom_hline(aes(yintercept = 0.92), col = "red", size = 1) +
  labs(x = "Test Set ID", y = "Accuracy")
```

## Motivation

If you construct a confidence interval and the error is large for a given test set size you know that your estimate is uncertain and should you increase your test set size.

If you construct two confidence intervals for two different models and the confidence intervals do not overlap then you know that one model is *significantly* better than the other. 

## Visualisation

```{r}
tibble(model = c("Model 1", "Model 2", "Model 1", "Model 2"), 
       metric = c(85, 89, 85, 89),
       lower = c(84, 88, 82, 86),
       upper = c(86, 90, 88, 92),
       study = c("Study 1 (error 1%)", "Study 1 (error 1%)", "Study 2 (error 3%)", "Study 2 (error 3%)")) %>% 
  ggplot() +
  geom_pointrange(aes(x = model, y = metric, ymin = lower, ymax = upper)) +
  facet_grid(~study) +
  labs(x = NULL, y = "Accuracy")
```

## Set up

For now assume that there are $n$ binary random variables $X_i\in\{0,1\}$ with expectation $\theta$ and the average of these binary random variables

$$
\hat{\theta}_n=\frac{1}{n}\sum_{i=1}^n{X_i}
$$

is used to estimate $\theta$.

## Example: Accuracy

For the accuracy metric the binary random variable $X_i$ indicates if the $i$th example is correctly classified ($X_i=0$) or incorrectly classified ($X_i=1$). $\hat{\theta}_n$ is the proportion of correctly classified examples on the test set and $\theta$ is the accuracy of the classifier.

## Hoeffding Confidence Interval

Hoeffding proved that

$$
\hat{\theta}_n\pm\epsilon
$$

is a $(1-\delta)$% confidence interval with test set size of $n$ if

$$
\epsilon=\sqrt{\frac{\ln(2/\delta)}{2n}}.
$$

For example, a 95% confidence interval (i.e. $\delta=0.05$) with a test set size of $n=1000$ has an error of approximately 4.29% with confidence interval $\hat{\theta}_{1000}\pm0.0429$

## Bound Issues

With small $n$ sometimes $\hat{\theta}_n-\epsilon<0$ or $\hat{\theta}_n+\epsilon>1$. In this case we can redefine the upper and lower bounds as

$$
[\max\{\hat{\theta}_n-\epsilon, 0\}, \min\{\hat{\theta}_n+\epsilon, 1\}]
$$
and this remains a $(1-\delta)$% confidence interval.

## Power Calculation

Hoeffding also proved that 

$$
\hat{\theta}_n\pm\epsilon
$$

is a $(1-\delta)$% confidence interval with error $\epsilon$ if 

$$
n\geq\frac{\ln(2/\delta)}{2\epsilon^2}.
$$

**This can be used to calculate the test set size for a given confidence $\delta$ and error $\epsilon$.**

## Application to Metrics

For different metrics $n$ has different values:

- **Accuracy**: number of examples
- **Precision**: number of examples classified as positive
- **Recall**: number of examples that are positive
- **Sensitivity**: number of examples that are positive
- **Specificity**: number of examples that are negative

## Code

This can all be done with a very simple python function:

```
def hoeffding_interval(n, delta, estimate):
    epsilon = math.sqrt((math.log(2 / delta)) / (2 * n))
    return (max(estimate - epsilon, 0), min(estimate + epsilon, 1))
```

For example if my estimate $\hat{\theta}_n=0.8$ then:

```
>>> import math
>>> hoeffding_interval(100, 0.05, 0.8)
(0.6641898484259381, 0.935810151574062)
```